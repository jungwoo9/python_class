{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CSR\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Users\\CSR\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "c:\\Users\\CSR\\anaconda3\\envs\\ml\\lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "# Import modules\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check gpu availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('data/traindata'), WindowsPath('data/testdata'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get train and test data directory path\n",
    "data_path = Path(\"./data\")\n",
    "train_path = data_path / \"traindata\" # get train data path\n",
    "test_path = data_path / \"testdata\" # get test data path\n",
    "\n",
    "train_path, test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Effnetb0 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained effnet model\n",
    "effnetb0_weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # get best weights\n",
    "effnetb0_transform = effnetb0_weights.transforms() # get transforms from weight\n",
    "effnetb0 = torchvision.models.efficientnet_b0(weights=effnetb0_weights).to(device) # get model with weights and send it to device\n",
    "\n",
    "# Freeze feature extractor layer\n",
    "for parameter in effnetb0.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Change classification layer\n",
    "effnetb0.classifier = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1280, out_features=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets\n",
    "train_data = datasets.ImageFolder(train_path, transform=effnetb0_transform)\n",
    "test_data = datasets.ImageFolder(test_path, transform=effnetb0_transform)\n",
    "\n",
    "# Get dataloader\n",
    "train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [32, 3, 256, 256]    [32, 2]              --                   Partial\n",
       "├─Sequential (features)                                      [32, 3, 256, 256]    [32, 1280, 8, 8]     --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [32, 3, 256, 256]    [32, 32, 128, 128]   --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 3, 256, 256]    [32, 32, 128, 128]   (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 32, 128, 128]   [32, 32, 128, 128]   (64)                 False\n",
       "│    │    └─SiLU (2)                                         [32, 32, 128, 128]   [32, 32, 128, 128]   --                   --\n",
       "│    └─Sequential (1)                                        [32, 32, 128, 128]   [32, 16, 128, 128]   --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 32, 128, 128]   [32, 16, 128, 128]   (1,448)              False\n",
       "│    └─Sequential (2)                                        [32, 16, 128, 128]   [32, 24, 64, 64]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 16, 128, 128]   [32, 24, 64, 64]     (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [32, 24, 64, 64]     [32, 24, 64, 64]     (10,710)             False\n",
       "│    └─Sequential (3)                                        [32, 24, 64, 64]     [32, 40, 32, 32]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 24, 64, 64]     [32, 40, 32, 32]     (15,350)             False\n",
       "│    │    └─MBConv (1)                                       [32, 40, 32, 32]     [32, 40, 32, 32]     (31,290)             False\n",
       "│    └─Sequential (4)                                        [32, 40, 32, 32]     [32, 80, 16, 16]     --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 40, 32, 32]     [32, 80, 16, 16]     (37,130)             False\n",
       "│    │    └─MBConv (1)                                       [32, 80, 16, 16]     [32, 80, 16, 16]     (102,900)            False\n",
       "│    │    └─MBConv (2)                                       [32, 80, 16, 16]     [32, 80, 16, 16]     (102,900)            False\n",
       "│    └─Sequential (5)                                        [32, 80, 16, 16]     [32, 112, 16, 16]    --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 80, 16, 16]     [32, 112, 16, 16]    (126,004)            False\n",
       "│    │    └─MBConv (1)                                       [32, 112, 16, 16]    [32, 112, 16, 16]    (208,572)            False\n",
       "│    │    └─MBConv (2)                                       [32, 112, 16, 16]    [32, 112, 16, 16]    (208,572)            False\n",
       "│    └─Sequential (6)                                        [32, 112, 16, 16]    [32, 192, 8, 8]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 112, 16, 16]    [32, 192, 8, 8]      (262,492)            False\n",
       "│    │    └─MBConv (1)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      (587,952)            False\n",
       "│    │    └─MBConv (2)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      (587,952)            False\n",
       "│    │    └─MBConv (3)                                       [32, 192, 8, 8]      [32, 192, 8, 8]      (587,952)            False\n",
       "│    └─Sequential (7)                                        [32, 192, 8, 8]      [32, 320, 8, 8]      --                   False\n",
       "│    │    └─MBConv (0)                                       [32, 192, 8, 8]      [32, 320, 8, 8]      (717,232)            False\n",
       "│    └─Conv2dNormActivation (8)                              [32, 320, 8, 8]      [32, 1280, 8, 8]     --                   False\n",
       "│    │    └─Conv2d (0)                                       [32, 320, 8, 8]      [32, 1280, 8, 8]     (409,600)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [32, 1280, 8, 8]     [32, 1280, 8, 8]     (2,560)              False\n",
       "│    │    └─SiLU (2)                                         [32, 1280, 8, 8]     [32, 1280, 8, 8]     --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [32, 1280, 8, 8]     [32, 1280, 1, 1]     --                   --\n",
       "├─Sequential (classifier)                                    [32, 1280]           [32, 2]              --                   True\n",
       "│    └─Linear (0)                                            [32, 1280]           [32, 2]              2,562                True\n",
       "============================================================================================================================================\n",
       "Total params: 4,010,110\n",
       "Trainable params: 2,562\n",
       "Non-trainable params: 4,007,548\n",
       "Total mult-adds (G): 16.07\n",
       "============================================================================================================================================\n",
       "Input size (MB): 25.17\n",
       "Forward/backward pass size (MB): 4508.12\n",
       "Params size (MB): 16.04\n",
       "Estimated Total Size (MB): 4549.33\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the model structure\n",
    "summary(effnetb0, \n",
    "        input_size=(32, 3, 256, 256), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a703761189ba4c9988dd1b06b8f19cfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train loss: 0.33, train_acc: 0.86 | test loss: 0.29, test_acc: 0.88\n",
      "\n",
      "Epoch 2\n",
      "train loss: 0.27, train_acc: 0.89 | test loss: 0.25, test_acc: 0.90\n",
      "\n",
      "Epoch 3\n",
      "train loss: 0.25, train_acc: 0.90 | test loss: 0.31, test_acc: 0.90\n",
      "\n",
      "Epoch 4\n",
      "train loss: 0.24, train_acc: 0.91 | test loss: 0.25, test_acc: 0.90\n",
      "\n",
      "Epoch 5\n",
      "train loss: 0.24, train_acc: 0.90 | test loss: 0.23, test_acc: 0.90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Get optimizer\n",
    "optimizer = torch.optim.Adam(effnetb0.parameters(), lr=0.001)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Set loss and accuracy list of both train and test \n",
    "train_loss_lst = []\n",
    "train_acc_lst = []\n",
    "test_loss_lst = []\n",
    "test_acc_lst = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Train model\n",
    "    effnetb0.train()\n",
    "\n",
    "    # Set train loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Iterate train dataloader\n",
    "    for X, y in train_dataloader:\n",
    "        # Send X and y to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = effnetb0(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred_class)\n",
    "    \n",
    "    # Get average loss and accuracy\n",
    "    train_loss /= len(train_dataloader)\n",
    "    train_acc /= len(train_dataloader)\n",
    "\n",
    "    # Append loss and accuracy to its list\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(train_acc)\n",
    "\n",
    "    # Evaluate model\n",
    "    effnetb0.eval()\n",
    "    \n",
    "    # Set test loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # With no updating weight\n",
    "    with torch.inference_mode():\n",
    "        # Iterate test dataloader\n",
    "        for X, y in test_dataloader:\n",
    "            # Send X and y to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred = effnetb0(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            test_pred_class = test_pred.argmax(dim=1)\n",
    "            test_acc += ((test_pred_class==y).sum().item()/len(test_pred_class))\n",
    "\n",
    "        # Get average loss and accuracy\n",
    "        test_loss /= len(test_dataloader)\n",
    "        test_acc /= len(test_dataloader)\n",
    "    \n",
    "    # Append loss and accuracy to its list\n",
    "    test_loss_lst.append(test_loss)\n",
    "    test_acc_lst.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\\ntrain loss: {train_loss:.2f}, train_acc: {train_acc:.2f} | test loss: {test_loss:.2f}, test_acc: {test_acc:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory to save model\n",
    "saving_path = Path(\"./model\")\n",
    "saving_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give file name(model name) and get path to save the model\n",
    "model_name = \"effnetb0.pth\"\n",
    "model_saving_path = saving_path / model_name\n",
    "\n",
    "# Save the model\n",
    "torch.save(obj=effnetb0.state_dict(), f=model_saving_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get pretrained effnet model\n",
    "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # get best weights\n",
    "vit_transform = vit_weights.transforms() # get transforms from weight\n",
    "vit = torchvision.models.vit_b_16(weights=vit_weights).to(device) # get model with weights and put it on cuda\n",
    "\n",
    "# Freeze feature extractor layer\n",
    "for parameter in vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# Change classification layer\n",
    "vit.heads = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=768, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 2]              768                  Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    (590,592)            False\n",
       "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              False\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       (7,087,872)          False\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       (1,536)              False\n",
       "├─Sequential (heads)                                         [32, 768]            [32, 2]              --                   True\n",
       "│    └─Linear (0)                                            [32, 768]            [32, 2]              1,538                True\n",
       "============================================================================================================================================\n",
       "Total params: 85,800,194\n",
       "Trainable params: 1,538\n",
       "Non-trainable params: 85,798,656\n",
       "Total mult-adds (G): 5.52\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.74\n",
       "Params size (MB): 229.20\n",
       "Estimated Total Size (MB): 3579.20\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the model structure\n",
    "summary(vit,\n",
    "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get datasets\n",
    "vit_train_data = datasets.ImageFolder(train_path, transform=vit_transform)\n",
    "vit_test_data = datasets.ImageFolder(test_path, transform=vit_transform)\n",
    "\n",
    "# Get dataloader\n",
    "vit_train_dataloader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "vit_test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e5b90e47484aab87e858a0beb124e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "train loss: 0.26, train_acc: 0.89 | test loss: 0.21, test_acc: 0.91\n",
      "\n",
      "Epoch 2\n",
      "train loss: 0.17, train_acc: 0.93 | test loss: 0.18, test_acc: 0.93\n",
      "\n",
      "Epoch 3\n",
      "train loss: 0.15, train_acc: 0.94 | test loss: 0.18, test_acc: 0.93\n",
      "\n",
      "Epoch 4\n",
      "train loss: 0.14, train_acc: 0.95 | test loss: 0.18, test_acc: 0.93\n",
      "\n",
      "Epoch 5\n",
      "train loss: 0.13, train_acc: 0.95 | test loss: 0.17, test_acc: 0.94\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Get optimizer\n",
    "optimizer = torch.optim.Adam(vit.parameters(), lr=0.001)\n",
    "\n",
    "# Set number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Set loss and accuracy list of both train and test \n",
    "train_loss_lst = []\n",
    "train_acc_lst = []\n",
    "test_loss_lst = []\n",
    "test_acc_lst = []\n",
    "\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    # Train model\n",
    "    vit.train()\n",
    "\n",
    "    # Set train loss and accuracy\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Iterate train dataloader\n",
    "    for X, y in vit_train_dataloader:\n",
    "        # Send X and y to device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = vit(X)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred_class)\n",
    "    \n",
    "    # Get average loss and accuracy\n",
    "    train_loss /= len(vit_train_dataloader)\n",
    "    train_acc /= len(vit_train_dataloader)\n",
    "\n",
    "    # Append loss and accuracy to its list\n",
    "    train_loss_lst.append(train_loss)\n",
    "    train_acc_lst.append(train_acc)\n",
    "\n",
    "    # Evaluate model\n",
    "    vit.eval()\n",
    "    \n",
    "    # Set test loss and accuracy\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # With no updating weight\n",
    "    with torch.inference_mode():\n",
    "        # Iterate test dataloader\n",
    "        for X, y in vit_test_dataloader:\n",
    "            # Send X and y to device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            test_pred = vit(X)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = loss_fn(test_pred, y)\n",
    "            test_loss += loss\n",
    "\n",
    "            # Calculate accuracy\n",
    "            test_pred_class = test_pred.argmax(dim=1)\n",
    "            test_acc += ((test_pred_class==y).sum().item()/len(test_pred_class))\n",
    "\n",
    "        # Get average loss and accuracy\n",
    "        test_loss /= len(vit_test_dataloader)\n",
    "        test_acc /= len(vit_test_dataloader)\n",
    "    \n",
    "    # Append loss and accuracy to its list\n",
    "    test_loss_lst.append(test_loss)\n",
    "    test_acc_lst.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\\ntrain loss: {train_loss:.2f}, train_acc: {train_acc:.2f} | test loss: {test_loss:.2f}, test_acc: {test_acc:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory to save model\n",
    "saving_path = Path(\"./model\")\n",
    "saving_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Give file name(model name) and get path to save the model\n",
    "model_name = \"vit.pth\"\n",
    "model_saving_path = saving_path / model_name\n",
    "\n",
    "# Save the model\n",
    "torch.save(obj=vit.state_dict(), f=model_saving_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
